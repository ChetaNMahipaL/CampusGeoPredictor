{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "from timm.data.mixup import Mixup\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import autoaugment\n",
    "from timm.data import RandAugment\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "import csv\n",
    "import timm\n",
    "from itertools import combinations\n",
    "from timm import create_model\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device_1 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    autoaugment.RandAugment(num_ops=2,magnitude=14),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ,\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_color = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_affine = T.Compose([\n",
    "    T.Resize((288, 288)),\n",
    "    T.RandomResizedCrop(256, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.85, 1.15), shear=10),\n",
    "    T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train & Val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            region = int(row['Region_ID'])\n",
    "            region = region - 1\n",
    "\n",
    "        region_tensor = torch.tensor(region, dtype=torch.long)\n",
    "        \n",
    "        return image, region_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Augment Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_dataset(image_dir, labels_df):\n",
    "    # Original dataset\n",
    "    original_dataset = RegionDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_base\n",
    "    )\n",
    "    \n",
    "    # Color jitter augmented dataset\n",
    "    color_dataset = RegionDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_color\n",
    "    )\n",
    "    \n",
    "    # Affine transform augmented dataset\n",
    "    affine_dataset = RegionDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_affine\n",
    "    )\n",
    "    \n",
    "    extended_dataset = ConcatDataset([original_dataset, color_dataset, affine_dataset])\n",
    "    \n",
    "    return extended_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = \"../../Dataset/Train/images_train\"\n",
    "labels_path_train = \"../../Dataset/Train/labels_train.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(labels_path_train)\n",
    "\n",
    "train_dataset = create_extended_dataset(image_dir_train, labels_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir_val = \"../../Dataset/Val/images_val\"\n",
    "labels_path_val = \"../../Dataset/Val/labels_val.csv\"\n",
    "labels_df_val = pd.read_csv(labels_path_val)\n",
    "\n",
    "val_dataset = RegionDataset(images_dir_val, labels_df_val, transform_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir_test = \"../../Dataset/Test\"\n",
    "\n",
    "test_dataset = TestDataset(images_dir_test, transform_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeitRegionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=15, model_name='deit_base_patch16_224'):\n",
    "        super(DeitRegionClassifier, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=True)\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "class GlobalContextModule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.conv_mask = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        self.channel_attn = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        \n",
    "        # Spatial attention\n",
    "        input_x = x\n",
    "        input_x = self.conv_mask(input_x)  # [B, 1, H, W]\n",
    "        input_x = input_x.view(batch, 1, height * width)\n",
    "        attn = self.softmax(input_x)  # Softmax along spatial dimension\n",
    "        attn = attn.view(batch, 1, height, width)\n",
    "        \n",
    "        # Apply spatial attention to feature map\n",
    "        x_weighted = x * attn\n",
    "        \n",
    "        # Global context\n",
    "        context = torch.sum(x_weighted, dim=(2, 3), keepdim=True) / (height * width)\n",
    "        context = context.view(batch, channels)\n",
    "        \n",
    "        # Channel attention\n",
    "        channel_attn = self.channel_attn(context).view(batch, channels, 1, 1)\n",
    "        \n",
    "        return x * channel_attn\n",
    "\n",
    "class FeaturePyramidModule(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels_list, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            for in_channels in in_channels_list\n",
    "        ])\n",
    "        \n",
    "        self.output_convs = nn.ModuleList([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "            for _ in range(len(in_channels_list))\n",
    "        ])\n",
    "        \n",
    "    def forward(self, features):\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        prev = self.lateral_convs[-1](features[-1])\n",
    "        results.append(self.output_convs[-1](prev))\n",
    "        \n",
    "        for i in range(len(features) - 2, -1, -1):\n",
    "\n",
    "            current = self.lateral_convs[i](features[i])\n",
    "            \n",
    "            prev_shape = prev.shape[2:]\n",
    "            current_shape = current.shape[2:]\n",
    "            \n",
    "            if prev_shape[0] < current_shape[0] or prev_shape[1] < current_shape[1]:\n",
    "                prev = F.interpolate(prev, size=current_shape, mode='bilinear', align_corners=False)\n",
    "            \n",
    "            prev = current + prev\n",
    "            results.append(self.output_convs[i](prev))\n",
    "        \n",
    "        return list(reversed(results))\n",
    "\n",
    "class SelfAttention2D(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)  # [B, N, C']\n",
    "        proj_key = self.key(x).view(B, -1, H * W)                       # [B, C', N]\n",
    "        energy = torch.bmm(proj_query, proj_key)                       # [B, N, N]\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        proj_value = self.value(x).view(B, -1, H * W)                  # [B, C, N]\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))       # [B, C, N]\n",
    "        out = out.view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "    \n",
    "class RegionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, dropout_rate=0.3, use_fpn=True, model=\"convex\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        if model == \"convex\":\n",
    "            self.backbone = timm.create_model(\n",
    "                'convnext_base',\n",
    "                pretrained=pretrained,\n",
    "                features_only=True,\n",
    "                out_indices=(1, 2, 3)\n",
    "            )\n",
    "        else:\n",
    "            self.backbone = timm.create_model(\n",
    "                'tf_efficientnetv2_m',\n",
    "                pretrained=True,\n",
    "                features_only=True,\n",
    "                out_indices=(2, 3, 4)\n",
    "            )\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy_input)\n",
    "            feature_channels = [f.shape[1] for f in features]\n",
    "            print(f\"Backbone feature channels: {feature_channels}\")\n",
    "        \n",
    "        self.use_fpn = use_fpn\n",
    "        if use_fpn:\n",
    "            self.fpn = FeaturePyramidModule(feature_channels, 256)\n",
    "            merged_channels = 256\n",
    "        else:\n",
    "            merged_channels = feature_channels[-1]\n",
    "\n",
    "        self.attention = SelfAttention2D(merged_channels)\n",
    "        self.gc_module = GlobalContextModule(merged_channels)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(merged_channels, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if self.use_fpn:\n",
    "            features = self.fpn(features)\n",
    "            feat = features[0]\n",
    "        else:\n",
    "            feat = features[-1]\n",
    "\n",
    "        feat = self.attention(feat)\n",
    "        feat = self.gc_module(feat)\n",
    "        \n",
    "        x = F.adaptive_avg_pool2d(feat, 1).flatten(1)\n",
    "        out = self.classifier(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None,\n",
    "    prob=0.8, switch_prob=0.3, mode='batch',\n",
    "    label_smoothing=0.1, num_classes=15\n",
    ")\n",
    "\n",
    "def train_classification_model(model, train_loader, val_loader, optimizer, num_epochs, device, model_path = \"best_model.pth\"):\n",
    "    # model = nn.DataParallel(model, device_ids=[0, 1])  # use GPUs 0 and 1\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Cosine LRS Scheduler\n",
    "    scheduler = CosineLRScheduler(\n",
    "        optimizer,\n",
    "        t_initial=num_epochs,\n",
    "        lr_min=1e-5,\n",
    "        cycle_mul=1.0,\n",
    "        warmup_t=5,\n",
    "        warmup_lr_init=1e-6,\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "\n",
    "        for images, targets in pbar:\n",
    "            images, targets = mixup_fn(images, targets)\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device) \n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if len(targets.shape) == 1:\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct_train += (preds == targets).sum().item()\n",
    "                total_train += targets.size(0)\n",
    "            else:\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                _, target_labels = torch.max(targets, dim=1) \n",
    "                correct_train += (preds == target_labels).sum().item()\n",
    "                total_train += targets.size(0)\n",
    "\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix(loss=loss.item(), acc=f\"{100*correct_train/total_train:.2f}%\")\n",
    "            pbar.update(1)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            for images, targets in pbar:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total_val += targets.size(0)\n",
    "                correct_val += (predicted == targets).sum().item()\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                pbar.set_postfix(loss=loss.item(), acc=f\"{100*correct_val/total_val:.2f}%\")\n",
    "                pbar.update(1)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_accuracy:.2f}%, \\\n",
    "                Val Loss = {avg_val_loss:.4f}, Val Acc = {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, image_tensor, device):\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            out = model(image_tensor)\n",
    "            logits.append(out)\n",
    "        avg_logits = torch.stack(logits).mean(dim=0)\n",
    "        avg_prob = torch.softmax(avg_logits, dim=1)\n",
    "        preds = torch.argmax(avg_prob, dim=1)\n",
    "    return preds, avg_prob\n",
    "\n",
    "\n",
    "def evaluate_classification_model(models, test_loader, device, num_regions=15):\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    confusion_matrix = torch.zeros(num_regions, num_regions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            preds, _ = ensemble_predict(models, inputs, device)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_ground_truths.extend(labels.cpu().numpy())\n",
    "\n",
    "            for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    per_class_accuracy = confusion_matrix.diag() / confusion_matrix.sum(1)\n",
    "    per_class_accuracy = per_class_accuracy.cpu().numpy()\n",
    "\n",
    "    adjacent_correct = 0\n",
    "    for pred, true in zip(all_predictions, all_ground_truths):\n",
    "        if pred == true or (pred == (true + 1) % num_regions) or (pred == (true - 1) % num_regions):\n",
    "            adjacent_correct += 1\n",
    "\n",
    "    adjacent_accuracy = 100 * adjacent_correct / total\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'adjacent_accuracy': adjacent_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'ground_truths': all_ground_truths\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_submission(models, val_loader, test_loader, device, output_csv_name='YourRollNo_VersionofSubmission.csv', num_regions=15):\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            preds, _ = ensemble_predict(models, inputs, device)\n",
    "            predictions.extend((preds + 1).cpu().tolist())\n",
    "\n",
    "        assert len(predictions) == 369, \"Expected 369 validation predictions.\"\n",
    "\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            preds, _ = ensemble_predict(models, inputs, device)\n",
    "            predictions.extend((preds + 1).cpu().tolist())\n",
    "\n",
    "        assert len(predictions) == 738, \"Total submission rows should be 738 (excluding header).\"\n",
    "\n",
    "    with open(output_csv_name, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['id', 'Region_ID'])\n",
    "        for idx, region_id in enumerate(predictions):\n",
    "            writer.writerow([idx, region_id])\n",
    "\n",
    "    print(f\"Submission file '{output_csv_name}' created successfully with {len(predictions)} predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 35\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convnext = RegionClassifier(num_classes=15, pretrained=True, dropout_rate=0.2, use_fpn=True, model = \"convex\")\n",
    "optimizer = optim.AdamW(\n",
    "    model_convnext.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.05\n",
    ")\n",
    "train_classification_model(\n",
    "    model_convnext, train_loader, val_loader, optimizer, num_epochs, device_1, model_path=\"model_convnext.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deit = DeitRegionClassifier(num_classes=15, model_name='deit_base_patch16_224')\n",
    "optimizer = optim.AdamW(\n",
    "    model_deit.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.05\n",
    ")\n",
    "train_classification_model(\n",
    "    model_deit, train_loader, val_loader, optimizer, num_epochs, device_1, model_path=\"model_deit.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convnext_ev = RegionClassifier(num_classes=15, pretrained=True, dropout_rate=0.2, use_fpn=True)\n",
    "model_deit_ev = DeitRegionClassifier(num_classes=15, model_name='deit_base_patch16_224')\n",
    "model_convnext_ev.load_state_dict(torch.load(\"model_convnext.pth\"))\n",
    "model_deit_ev.load_state_dict(torch.load(\"model_deit.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "models = [ model_deit_ev, model_convnext_ev] \n",
    "# best_result = {\n",
    "#     'models': None,\n",
    "#     'accuracy': 0,\n",
    "#     'adjacent_accuracy': 0\n",
    "# }\n",
    "\n",
    "# # Try every non-empty combination of models\n",
    "# for r in range(1, len(models) + 1):\n",
    "#     for combo in combinations(models, r):\n",
    "#         combo_list = list(combo)\n",
    "#         results = evaluate_classification_model(combo_list, val_loader, device_1, num_regions=15)\n",
    "#         print(f\"Models: {[m.__class__.__name__ for m in combo_list]} | \"\n",
    "#               f\"Accuracy: {results['accuracy']:.2f}% | \"\n",
    "#               f\"Adjacent Accuracy: {results['adjacent_accuracy']:.2f}%\")\n",
    "\n",
    "#         if results['accuracy'] > best_result['accuracy']:\n",
    "#             best_result = {\n",
    "#                 'models': combo_list,\n",
    "#                 'accuracy': results['accuracy'],\n",
    "#                 'adjacent_accuracy': results['adjacent_accuracy']\n",
    "#             }\n",
    "\n",
    "# # Print best result\n",
    "# print(\"\\nBest Combination:\")\n",
    "# print(f\"Models: {[m.__class__.__name__ for m in best_result['models']]}\")\n",
    "# print(f\"Accuracy: {best_result['accuracy']:.2f}%\")\n",
    "# print(f\"Adjacent Accuracy: {best_result['adjacent_accuracy']:.2f}%\")\n",
    "# Evaluate Ensemble Model\n",
    "\n",
    "models = [model_convnext_ev, model_deit_ev]\n",
    "results = evaluate_classification_model(models, val_loader, device_1, num_regions=15)\n",
    "print(f\"Accuracy: {results['accuracy']:.2f}%\")\n",
    "print(f\"Adjacent Accuracy: {results['adjacent_accuracy']:.2f}%\")\n",
    "generate_submission(models, val_loader, test_loader, device_1, output_csv_name='2022101096_5.csv', num_regions=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
