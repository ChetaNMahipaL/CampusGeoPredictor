{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a456899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df=None, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.filenames = labels_df['filename'] if labels_df is not None else [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        \n",
    "        label = int(row['Region_ID'])\n",
    "            \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label, self.filenames[idx] if isinstance(self.filenames, list) else row['filename']\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        resnet = models.resnet50(weights='DEFAULT' if pretrained else None)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        num_features = resnet.fc.in_features\n",
    "        \n",
    "        self.fc = nn.Linear(num_features, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, output_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple decoder using transposed convolutions\n",
    "        self.fc = nn.Linear(latent_dim, 512*4*4)  # Expand to initial feature map\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 32x32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 64x64\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # 128x128\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, output_channels, kernel_size=4, stride=2, padding=1),  # 256x256\n",
    "            nn.Sigmoid()  # Scale output to [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim, pretrained)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs, device, learning_rate=1e-4):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for images, _, _ in pbar:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            reconstructed, _ = model(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            for images, _, _ in pbar:\n",
    "                images = images.to(device)\n",
    "                \n",
    "                reconstructed, _ = model(images)\n",
    "                loss = criterion(reconstructed, images)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_autoencoder.pth')\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_features(model, data_loader, device):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels, batch_filenames in tqdm(data_loader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            latent_features = model.encode(images)\n",
    "            \n",
    "            features.append(latent_features.cpu().numpy())\n",
    "            labels.append(batch_labels.numpy())\n",
    "            filenames.extend(batch_filenames)\n",
    "    \n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    return features, labels, filenames\n",
    "\n",
    "def cluster_features(features, n_clusters=15):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features)\n",
    "    return kmeans, cluster_labels\n",
    "\n",
    "def map_clusters_to_regions(cluster_labels, true_labels, n_clusters=15, n_regions=15):\n",
    "\n",
    "    mapping = {}\n",
    "    for cluster in range(n_clusters):\n",
    "\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "\n",
    "        cluster_true_labels = true_labels[cluster_indices]\n",
    "        \n",
    "\n",
    "        unique_labels, counts = np.unique(cluster_true_labels, return_counts=True)\n",
    "        \n",
    "        if len(unique_labels) == 1 and unique_labels[0] == -1:\n",
    "            continue\n",
    "            \n",
    "        valid_indices = unique_labels != -1\n",
    "        valid_unique = unique_labels[valid_indices]\n",
    "        valid_counts = counts[valid_indices]\n",
    "        \n",
    "        if len(valid_unique) > 0:\n",
    "\n",
    "            most_frequent_region = valid_unique[np.argmax(valid_counts)]\n",
    "            mapping[cluster] = most_frequent_region\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def evaluate_clustering(cluster_labels, true_labels, cluster_to_region_map, n_regions=15):\n",
    "\n",
    "    valid_indices = true_labels != -1\n",
    "    \n",
    "    if np.sum(valid_indices) == 0:\n",
    "        return {\n",
    "            'accuracy': 0,\n",
    "            'adjacent_accuracy': 0,\n",
    "            'confusion_matrix': np.zeros((n_regions, n_regions))\n",
    "        }\n",
    "    \n",
    "    valid_clusters = cluster_labels[valid_indices]\n",
    "    valid_true = true_labels[valid_indices]\n",
    "    \n",
    "    predicted_regions = np.array([cluster_to_region_map.get(c, -1) for c in valid_clusters])\n",
    "    \n",
    "    eval_indices = predicted_regions != -1\n",
    "    pred_regions = predicted_regions[eval_indices]\n",
    "    true_regions = valid_true[eval_indices]\n",
    "    \n",
    "    if len(pred_regions) == 0:\n",
    "        return {\n",
    "            'accuracy': 0,\n",
    "            'adjacent_accuracy': 0,\n",
    "            'confusion_matrix': np.zeros((n_regions, n_regions))\n",
    "        }\n",
    "    \n",
    "    correct = np.sum(pred_regions == true_regions)\n",
    "    accuracy = 100 * correct / len(pred_regions)\n",
    "    \n",
    "    adjacent_correct = 0\n",
    "    for i in range(len(pred_regions)):\n",
    "        pred = pred_regions[i]\n",
    "        true = true_regions[i]\n",
    "\n",
    "        if pred == true or (pred == (true + 1) % n_regions) or (pred == (true - 1) % n_regions):\n",
    "            adjacent_correct += 1\n",
    "    \n",
    "    adjacent_accuracy = 100 * adjacent_correct / len(pred_regions)\n",
    "    \n",
    "    confusion_matrix = np.zeros((n_regions, n_regions))\n",
    "    for t, p in zip(true_regions, pred_regions):\n",
    "        confusion_matrix[t, p] += 1\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'adjacent_accuracy': adjacent_accuracy,\n",
    "        'confusion_matrix': confusion_matrix\n",
    "    }\n",
    "\n",
    "def autoencoder_clustering_pipeline(train_loader, val_loader, test_loader, latent_dim=128, \n",
    "                                   n_clusters=15, n_regions=15, num_epochs=10, device=\"cuda\"):\n",
    "\n",
    "    print(\"Training autoencoder...\")\n",
    "    model = Autoencoder(latent_dim=latent_dim)\n",
    "    trained_model = train_autoencoder(model, train_loader, val_loader, num_epochs, device)\n",
    "    \n",
    "    print(\"Extracting features from training data...\")\n",
    "    train_features, train_labels, _ = extract_features(trained_model, train_loader, device)\n",
    "    \n",
    "\n",
    "    print(\"Clustering features...\")\n",
    "    kmeans, cluster_labels = cluster_features(train_features, n_clusters=n_clusters)\n",
    "    \n",
    "    print(\"Mapping clusters to regions...\")\n",
    "    cluster_to_region_map = map_clusters_to_regions(cluster_labels, train_labels, \n",
    "                                                   n_clusters=n_clusters, n_regions=n_regions)\n",
    "    \n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_features, test_labels, test_filenames = extract_features(trained_model, test_loader, device)\n",
    "    test_cluster_labels = kmeans.predict(test_features)\n",
    "    \n",
    "    results = evaluate_clustering(test_cluster_labels, test_labels, cluster_to_region_map, n_regions=n_regions)\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': trained_model.state_dict(),\n",
    "        'kmeans': kmeans,\n",
    "        'cluster_to_region_map': cluster_to_region_map,\n",
    "        'latent_dim': latent_dim,\n",
    "        'n_clusters': n_clusters\n",
    "    }, 'autoencoder_clustering_model.pth')\n",
    "    \n",
    "    return trained_model, kmeans, cluster_to_region_map, results\n",
    "\n",
    "def predict_region(model, kmeans, cluster_to_region_map, image_tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "        latent = model.encode(image_tensor)\n",
    "        latent_np = latent.cpu().numpy()\n",
    "        \n",
    "        cluster = kmeans.predict(latent_np)[0]\n",
    "        \n",
    "        region = cluster_to_region_map.get(cluster, -1)\n",
    "        if region != -1:\n",
    "            region = region + 1\n",
    "        \n",
    "    return region\n",
    "\n",
    "transform_base = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ,\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_dir_train = \"Dataset/Train/images_train\"\n",
    "labels_path_train = \"Dataset/Train/labels_train.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(labels_path_train)\n",
    "\n",
    "train_dataset = AngleDataset(image_dir_train, labels_df,transform=transform_base)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "images_dir_val = \"Dataset/Val/images_val\"\n",
    "labels_path_val = \"Dataset/Val/labels_val.csv\"\n",
    "labels_df_val = pd.read_csv(labels_path_val)\n",
    "\n",
    "val_dataset = AngleDataset(images_dir_val, labels_df_val, transform_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, kmeans, cluster_map, results = autoencoder_clustering_pipeline(\n",
    "    train_loader, val_loader, val_loader, \n",
    "    latent_dim=128, n_clusters=15, n_regions=15,\n",
    "    num_epochs=10, device=device\n",
    ")\n",
    "\n",
    "print(f\"Test accuracy: {results['accuracy']:.2f}%\")\n",
    "print(f\"Adjacent accuracy: {results['adjacent_accuracy']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
