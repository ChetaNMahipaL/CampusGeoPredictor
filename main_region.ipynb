{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "from PIL import ImageFilter, ImageEnhance\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "from timm.data.mixup import Mixup\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import autoaugment\n",
    "from timm.data import RandAugment\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    autoaugment.RandAugment(num_ops=2,magnitude=9),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ,\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_color = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_affine = T.Compose([\n",
    "    T.Resize((288, 288)),\n",
    "    T.RandomResizedCrop(256, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.85, 1.15), shear=10),\n",
    "    T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            region = int(row['Region_ID'])\n",
    "            region = region - 1\n",
    "\n",
    "        # Convert to tensor with proper dtype (long) for classification tasks\n",
    "        region_tensor = torch.tensor(region, dtype=torch.long)\n",
    "        \n",
    "        return image, region_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_dataset(image_dir, labels_df):\n",
    "    # Original dataset\n",
    "    original_dataset = RegionDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_base\n",
    "    )\n",
    "    \n",
    "    # Color jitter augmented dataset\n",
    "    color_dataset = RegionDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_color\n",
    "    )\n",
    "    \n",
    "    # # Affine transform augmented dataset\n",
    "    affine_dataset = RegionDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_affine\n",
    "    )\n",
    "    \n",
    "    extended_dataset = ConcatDataset([original_dataset, color_dataset, affine_dataset])\n",
    "    \n",
    "    return extended_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = \"Dataset/Train/images_train\"\n",
    "labels_path_train = \"Dataset/Train/labels_train.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(labels_path_train)\n",
    "\n",
    "train_dataset = create_extended_dataset(image_dir_train, labels_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir_val = \"Dataset/Val/images_val\"\n",
    "labels_path_val = \"Dataset/Val/labels_val.csv\"\n",
    "labels_df_val = pd.read_csv(labels_path_val)\n",
    "\n",
    "val_dataset = RegionDataset(images_dir_val, labels_df_val, transform_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetRegionClassifier(nn.Module):\n",
    "    def __init__(self, num_regions=8, pretrained=True, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        base_model = models.resnet101(weights='DEFAULT' if pretrained else None)\n",
    "        num_features = base_model.fc.in_features\n",
    "\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.BatchNorm1d(1024), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, num_regions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_fn = Mixup(\n",
    "    mixup_alpha=0.8, cutmix_alpha=1.0, cutmix_minmax=None,\n",
    "    prob=0.8, switch_prob=0.3, mode='batch',\n",
    "    label_smoothing=0.1, num_classes=15\n",
    ")\n",
    "\n",
    "def train_classification_model(model, train_loader, val_loader, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Cosine LRS Scheduler\n",
    "    scheduler = CosineLRScheduler(\n",
    "        optimizer,\n",
    "        t_initial=num_epochs,\n",
    "        lr_min=1e-5,\n",
    "        cycle_mul=1.0,\n",
    "        warmup_t=5,\n",
    "        warmup_lr_init=1e-6,\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "\n",
    "        for images, targets in pbar:\n",
    "            images, targets = mixup_fn(images, targets)\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device) \n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if len(targets.shape) == 1:\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                correct_train = (preds == targets).sum().item()\n",
    "                total_train = targets.size(0)\n",
    "            else:\n",
    "                _, preds = torch.max(logits, dim=1)\n",
    "                _, target_labels = torch.max(targets, dim=1) \n",
    "                correct_train = (preds == target_labels).sum().item()\n",
    "                total_train = targets.size(0)\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix(loss=loss.item(), acc=f\"{100*correct_train/total_train:.2f}%\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            for images, targets in pbar:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                logits = model(images)\n",
    "                loss = loss_fn(logits, targets)\n",
    "\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total_val += targets.size(0)\n",
    "                correct_val += (predicted == targets).sum().item()\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                pbar.set_postfix(loss=loss.item(), acc=f\"{100*correct_val/total_val:.2f}%\")\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "        \n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_accuracy:.2f}%, \\\n",
    "                Val Loss = {avg_val_loss:.4f}, Val Acc = {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_model(model, test_loader, device, num_regions=15):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    confusion_matrix = torch.zeros(num_regions, num_regions)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_ground_truths.extend(labels.cpu().numpy())\n",
    "            \n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    per_class_accuracy = confusion_matrix.diag() / confusion_matrix.sum(1)\n",
    "    per_class_accuracy = per_class_accuracy.cpu().numpy()\n",
    "    \n",
    "    adjacent_correct = 0\n",
    "    for i in range(len(all_predictions)):\n",
    "        pred = all_predictions[i]\n",
    "        true = all_ground_truths[i]\n",
    "\n",
    "        if pred == true or (pred == (true + 1) % num_regions) or (pred == (true - 1) % num_regions):\n",
    "            adjacent_correct += 1\n",
    "    \n",
    "    adjacent_accuracy = 100 * adjacent_correct / total\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'adjacent_accuracy': adjacent_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'ground_truths': all_ground_truths\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to /home/chetan/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth\n",
      "100%|██████████| 171M/171M [00:16<00:00, 10.7MB/s] \n",
      "Epoch 1/20 [Train]: 100%|██████████| 2454/2454 [12:02<00:00,  3.39it/s, acc=50.00%, loss=2.74]\n",
      "Epoch 1/20 [Val]: 100%|██████████| 47/47 [00:04<00:00,  9.89it/s, acc=14.91%, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.7381, Train Acc = 50.00%,                 Val Loss = 2.6321, Val Acc = 14.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 2454/2454 [11:58<00:00,  3.42it/s, acc=0.00%, loss=2.7]  \n",
      "Epoch 2/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.75it/s, acc=19.24%, loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 2.6530, Train Acc = 0.00%,                 Val Loss = 2.5393, Val Acc = 19.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 2454/2454 [11:57<00:00,  3.42it/s, acc=0.00%, loss=3.13]  \n",
      "Epoch 3/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s, acc=62.06%, loss=0.546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 2.3024, Train Acc = 0.00%,                 Val Loss = 1.2742, Val Acc = 62.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 2454/2454 [12:05<00:00,  3.38it/s, acc=50.00%, loss=1.61]  \n",
      "Epoch 4/20 [Val]: 100%|██████████| 47/47 [00:04<00:00,  9.96it/s, acc=82.93%, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 1.9059, Train Acc = 50.00%,                 Val Loss = 0.7279, Val Acc = 82.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 2454/2454 [12:04<00:00,  3.39it/s, acc=50.00%, loss=2.56]  \n",
      "Epoch 5/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.19it/s, acc=89.16%, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 1.6972, Train Acc = 50.00%,                 Val Loss = 0.6142, Val Acc = 89.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 2454/2454 [11:59<00:00,  3.41it/s, acc=100.00%, loss=1.7]  \n",
      "Epoch 6/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s, acc=92.14%, loss=0.404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 1.5919, Train Acc = 100.00%,                 Val Loss = 0.4669, Val Acc = 92.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 2454/2454 [12:00<00:00,  3.40it/s, acc=100.00%, loss=1.12] \n",
      "Epoch 7/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.19it/s, acc=89.70%, loss=0.281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 1.5472, Train Acc = 100.00%,                 Val Loss = 0.5035, Val Acc = 89.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 2454/2454 [11:59<00:00,  3.41it/s, acc=0.00%, loss=2.72]   \n",
      "Epoch 8/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.18it/s, acc=92.95%, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 1.4507, Train Acc = 0.00%,                 Val Loss = 0.4512, Val Acc = 92.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 2454/2454 [11:56<00:00,  3.43it/s, acc=100.00%, loss=2.08] \n",
      "Epoch 9/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s, acc=92.41%, loss=0.329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 1.4083, Train Acc = 100.00%,                 Val Loss = 0.4672, Val Acc = 92.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 2454/2454 [12:00<00:00,  3.40it/s, acc=50.00%, loss=3.37]  \n",
      "Epoch 10/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.18it/s, acc=96.21%, loss=0.327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 1.3593, Train Acc = 50.00%,                 Val Loss = 0.4125, Val Acc = 96.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 2454/2454 [12:00<00:00,  3.41it/s, acc=100.00%, loss=2.96] \n",
      "Epoch 11/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s, acc=94.85%, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 1.3508, Train Acc = 100.00%,                 Val Loss = 0.4305, Val Acc = 94.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 2454/2454 [11:57<00:00,  3.42it/s, acc=50.00%, loss=1.54]  \n",
      "Epoch 12/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.28it/s, acc=94.31%, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 1.3108, Train Acc = 50.00%,                 Val Loss = 0.4942, Val Acc = 94.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|██████████| 2454/2454 [11:56<00:00,  3.42it/s, acc=100.00%, loss=0.855]\n",
      "Epoch 13/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.30it/s, acc=95.39%, loss=0.331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 1.2708, Train Acc = 100.00%,                 Val Loss = 0.4810, Val Acc = 95.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|██████████| 2454/2454 [11:55<00:00,  3.43it/s, acc=0.00%, loss=2.7]    \n",
      "Epoch 14/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.26it/s, acc=96.21%, loss=0.418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 1.2695, Train Acc = 0.00%,                 Val Loss = 0.4455, Val Acc = 96.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|██████████| 2454/2454 [11:55<00:00,  3.43it/s, acc=100.00%, loss=1.59] \n",
      "Epoch 15/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s, acc=97.02%, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 1.2555, Train Acc = 100.00%,                 Val Loss = 0.4196, Val Acc = 97.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|██████████| 2454/2454 [11:55<00:00,  3.43it/s, acc=100.00%, loss=2.23] \n",
      "Epoch 16/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.31it/s, acc=95.66%, loss=0.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 1.2348, Train Acc = 100.00%,                 Val Loss = 0.3591, Val Acc = 95.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|██████████| 2454/2454 [11:52<00:00,  3.44it/s, acc=100.00%, loss=1.62] \n",
      "Epoch 17/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.21it/s, acc=94.85%, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 1.2261, Train Acc = 100.00%,                 Val Loss = 0.4789, Val Acc = 94.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|██████████| 2454/2454 [11:49<00:00,  3.46it/s, acc=0.00%, loss=3.3]    \n",
      "Epoch 18/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.43it/s, acc=94.85%, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 1.2226, Train Acc = 0.00%,                 Val Loss = 0.4213, Val Acc = 94.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|██████████| 2454/2454 [11:38<00:00,  3.51it/s, acc=50.00%, loss=2.67]  \n",
      "Epoch 19/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.49it/s, acc=95.66%, loss=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 1.2116, Train Acc = 50.00%,                 Val Loss = 0.5192, Val Acc = 95.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|██████████| 2454/2454 [11:39<00:00,  3.51it/s, acc=100.00%, loss=0.713]\n",
      "Epoch 20/20 [Val]: 100%|██████████| 47/47 [00:04<00:00, 10.55it/s, acc=94.31%, loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 1.1935, Train Acc = 100.00%,                 Val Loss = 0.3939, Val Acc = 94.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ResNetRegionClassifier(num_regions=15, pretrained=True, dropout_rate=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 20\n",
    "train_classification_model(model, train_loader, val_loader, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.31%\n",
      "Adjacent Accuracy: 97.83%\n",
      "Per-Class Accuracy: [0.9047619  1.         0.9259259  0.962963   0.9259259  0.962963\n",
      " 1.         1.         0.8        0.93939394 0.875      0.9259259\n",
      " 0.9583333  1.         0.9444444 ]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "results = evaluate_classification_model(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {results['accuracy']:.2f}%\")\n",
    "print(f\"Adjacent Accuracy: {results['adjacent_accuracy']:.2f}%\")\n",
    "print(f\"Per-Class Accuracy: {results['per_class_accuracy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.02%\n",
      "Adjacent Accuracy: 98.92%\n",
      "Per-Class Accuracy: [0.95238096 1.         0.962963   1.         0.962963   1.\n",
      " 1.         0.9259259  0.93333334 0.969697   0.9166667  0.9259259\n",
      " 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Load the best model for evaluation\n",
    "\n",
    "model_ev = ResNetRegionClassifier(num_regions=15, pretrained=True, dropout_rate=0.1)\n",
    "model_ev.to(device)\n",
    "model_ev.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "results = evaluate_classification_model(model_ev, val_loader, device)\n",
    "print(f\"Validation Accuracy: {results['accuracy']:.2f}%\")\n",
    "print(f\"Adjacent Accuracy: {results['adjacent_accuracy']:.2f}%\")\n",
    "print(f\"Per-Class Accuracy: {results['per_class_accuracy']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
