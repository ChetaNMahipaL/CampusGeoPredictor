{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from torchvision.models.vision_transformer import vit_b_16\n",
    "import csv\n",
    "import timm\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "import csv\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandAugment(num_ops=2, magnitude=9),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ,\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_color = T.Compose([\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_affine = T.Compose([\n",
    "    T.Resize((288, 288)),\n",
    "    T.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.85, 1.15), shear=10),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def detect_outliers(df, columns, iqr_multiplier=1.35):\n",
    "    mask = np.ones(len(df), dtype=bool)\n",
    "\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - iqr_multiplier * IQR\n",
    "        upper_bound = Q3 + iqr_multiplier * IQR\n",
    "\n",
    "        col_mask = (df[col] >= lower_bound) & (df[col] <= upper_bound)\n",
    "        mask &= col_mask\n",
    "\n",
    "    return mask \n",
    "\n",
    "def preprocess_coordinates(df, stats=None, ids_to_remove=None):\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if df_clean['latitude'].isnull().sum() > 0 or df_clean['longitude'].isnull().sum() > 0:\n",
    "        print(f\"Warning: Found {df_clean['latitude'].isnull().sum()} missing latitude values and \"\n",
    "              f\"{df_clean['longitude'].isnull().sum()} missing longitude values\")\n",
    "        df_clean['latitude'] = df_clean['latitude'].fillna(df_clean['latitude'].median())\n",
    "        df_clean['longitude'] = df_clean['longitude'].fillna(df_clean['longitude'].median())\n",
    "    \n",
    "    if stats is None:\n",
    "        outlier_mask = detect_outliers(df_clean, ['latitude', 'longitude'])\n",
    "        outliers_count = (~outlier_mask).sum()\n",
    "        if outliers_count > 0:\n",
    "            print(f\"Removed {outliers_count} outliers from dataset\")\n",
    "            df_clean = df_clean[outlier_mask].reset_index(drop=True)\n",
    "        \n",
    "        df_clean['original_latitude'] = df_clean['latitude']\n",
    "        df_clean['original_longitude'] = df_clean['longitude']\n",
    "\n",
    "    if stats is not None:\n",
    "        lat_mean = stats['lat_mean']\n",
    "        lat_std = stats['lat_std']\n",
    "        lon_mean = stats['lon_mean']\n",
    "        lon_std = stats['lon_std']\n",
    "\n",
    "        normstats = {\n",
    "            'lat_mean': lat_mean,\n",
    "            'lat_std': lat_std,\n",
    "            'lon_mean': lon_mean,\n",
    "            'lon_std': lon_std\n",
    "        }\n",
    "    else:\n",
    "        lat_mean = df_clean['latitude'].mean()\n",
    "        lat_std = df_clean['latitude'].std()\n",
    "        lon_mean = df_clean['longitude'].mean()\n",
    "        lon_std = df_clean['longitude'].std()\n",
    "\n",
    "        normstats = {\n",
    "            'lat_mean': lat_mean,\n",
    "            'lat_std': lat_std,\n",
    "            'lon_mean': lon_mean,\n",
    "            'lon_std': lon_std\n",
    "        }\n",
    "\n",
    "    # Store stats in dataframe attributes\n",
    "    df_clean.attrs['lat_mean'] = lat_mean\n",
    "    df_clean.attrs['lat_std'] = lat_std\n",
    "    df_clean.attrs['lon_mean'] = lon_mean\n",
    "    df_clean.attrs['lon_std'] = lon_std\n",
    "\n",
    "    # Normalize using the specified statistics\n",
    "    df_clean['latitude'] = (df_clean['latitude'] - lat_mean) / lat_std\n",
    "    df_clean['longitude'] = (df_clean['longitude'] - lon_mean) / lon_std\n",
    "\n",
    "    return df_clean, normstats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Train & Val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df, transform=None, use_original_coords=False):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.use_original_coords = use_original_coords\n",
    "        \n",
    "        # Get stats for denormalization\n",
    "        self.lat_mean = labels_df.attrs.get('lat_mean')\n",
    "        self.lat_std = labels_df.attrs.get('lat_std')\n",
    "        self.lon_mean = labels_df.attrs.get('lon_mean')\n",
    "        self.lon_std = labels_df.attrs.get('lon_std')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.use_original_coords and 'original_latitude' in row:\n",
    "            lat = float(row['original_latitude'])\n",
    "            lon = float(row['original_longitude'])\n",
    "        else:\n",
    "            lat = float(row['latitude'])\n",
    "            lon = float(row['longitude'])  \n",
    "        \n",
    "        coords = torch.tensor([lat, lon], dtype=torch.float32)\n",
    "        return image, coords\n",
    "\n",
    "    def denormalize_coordinates(self, normalized_coords):\n",
    "        if None in (self.lat_mean, self.lat_std, self.lon_mean, self.lon_std):\n",
    "            return normalized_coords  # Return as-is if stats are missing\n",
    "\n",
    "        lat = normalized_coords[:, 0] * self.lat_std + self.lat_mean\n",
    "        lon = normalized_coords[:, 1] * self.lon_std + self.lon_mean\n",
    "\n",
    "        return torch.stack([lat, lon], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "        # Get stats for denormalization\n",
    "        self.lat_mean = labels_df.attrs.get('lat_mean')\n",
    "        self.lat_std = labels_df.attrs.get('lat_std')\n",
    "        self.lon_mean = labels_df.attrs.get('lon_mean')\n",
    "        self.lon_std = labels_df.attrs.get('lon_std')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    def denormalize_coordinates(self, normalized_coords):\n",
    "        if None in (self.lat_mean, self.lat_std, self.lon_mean, self.lon_std):\n",
    "            return normalized_coords  # Return as-is if stats are missing\n",
    "\n",
    "        lat = normalized_coords[:, 0] * self.lat_std + self.lat_mean\n",
    "        lon = normalized_coords[:, 1] * self.lon_std + self.lon_mean\n",
    "\n",
    "        return torch.stack([lat, lon], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Extend Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extended_dataset(image_dir, labels_df):\n",
    "    # Original dataset\n",
    "    original_dataset = GeoDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_base\n",
    "    )\n",
    "    \n",
    "    # Color jitter augmented dataset\n",
    "    color_dataset = GeoDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_color\n",
    "    )\n",
    "    \n",
    "    # Affine transform augmented dataset\n",
    "    affine_dataset = GeoDataset(\n",
    "        image_dir=image_dir,\n",
    "        labels_df=labels_df,\n",
    "        transform=transform_affine\n",
    "    )\n",
    "    \n",
    "    extended_dataset = ConcatDataset([original_dataset, affine_dataset])\n",
    "    \n",
    "    return extended_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 81 outliers from dataset\n"
     ]
    }
   ],
   "source": [
    "image_dir_train = \"Dataset/Train/images_train\"\n",
    "labels_path_train = \"Dataset/Train/labels_train.csv\"\n",
    "labels_df = pd.read_csv(labels_path_train)\n",
    "\n",
    "labels_df, normstats = preprocess_coordinates(labels_df)\n",
    "\n",
    "train_dataset = create_extended_dataset(image_dir_train, labels_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir_val = \"Dataset/Val/images_val\"\n",
    "labels_path_val = \"Dataset/Val/labels_val.csv\"\n",
    "labels_df_val = pd.read_csv(labels_path_val)\n",
    "\n",
    "labels_df_val, _ = preprocess_coordinates(labels_df_val, normstats)\n",
    "\n",
    "val_dataset = GeoDataset(images_dir_val, labels_df_val, transform_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir_test = \"./Dataset/Test\"\n",
    "\n",
    "test_dataset = TestDataset(images_dir_test,labels_df_val, transform_val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoSwinModel(nn.Module):\n",
    "    def __init__(self, model_name='swin_base_patch4_window7_224', dropout_rate=0.2, pretrained=True):\n",
    "        super(GeoSwinModel, self).__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0  # Remove classification head\n",
    "        )\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(self.feature_dim),\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)  # Shape: (B, feature_dim)\n",
    "        coords = self.regressor(features)\n",
    "        return {\"coords\": coords}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoLoss(nn.Module):\n",
    "    def __init__(self, lat_weight=0.5, lon_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.lat_weight = lat_weight\n",
    "        self.lon_weight = lon_weight\n",
    "        \n",
    "    def forward(self, pred, true):\n",
    "\n",
    "        if isinstance(pred, dict):\n",
    "            pred = pred[\"coords\"]\n",
    "            \n",
    "        pred_lat, pred_lon = pred[:, 0], pred[:, 1]\n",
    "        true_lat, true_lon = true[:, 0], true[:, 1]\n",
    "        \n",
    "        lat_loss = F.mse_loss(pred_lat, true_lat)\n",
    "        lon_loss = F.mse_loss(pred_lon, true_lon)\n",
    "        \n",
    "        total_loss = self.lat_weight * lat_loss + self.lon_weight * lon_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # For original scale metrics\n",
    "    total_orig_lat_mse = 0.0\n",
    "    total_orig_lon_mse = 0.0\n",
    "    combined_orig_mse = 0.0\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    all_pred_coords = []\n",
    "    all_true_coords = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, coords in test_loader:\n",
    "            inputs, coords = inputs.to(device), coords.to(device)\n",
    "            outputs = model(inputs)\n",
    "            pred_coords = outputs['coords']\n",
    "            \n",
    "            \n",
    "            if hasattr(test_loader.dataset, 'denormalize_coordinates'):\n",
    "                if isinstance(test_loader.dataset, ConcatDataset):\n",
    "\n",
    "                    orig_pred = test_loader.dataset.datasets[0].denormalize_coordinates(pred_coords.cpu())\n",
    "                    orig_true = test_loader.dataset.datasets[0].denormalize_coordinates(coords.cpu())\n",
    "                else:\n",
    "                    orig_pred = test_loader.dataset.denormalize_coordinates(pred_coords.cpu())\n",
    "                    orig_true = test_loader.dataset.denormalize_coordinates(coords.cpu())\n",
    "                \n",
    "                orig_lat_mse = F.mse_loss(orig_pred[:, 0], orig_true[:, 0], reduction='sum')\n",
    "                orig_lon_mse = F.mse_loss(orig_pred[:, 1], orig_true[:, 1], reduction='sum')\n",
    "                \n",
    "                total_orig_lat_mse += orig_lat_mse.item()\n",
    "                total_orig_lon_mse += orig_lon_mse.item()\n",
    "                combined_orig_mse += (orig_lat_mse.item() + orig_lon_mse.item()) / 2\n",
    "                \n",
    "                all_pred_coords.append(orig_pred.numpy())\n",
    "                all_true_coords.append(orig_true.numpy())\n",
    "            else:\n",
    "                all_pred_coords.append(pred_coords.cpu().numpy())\n",
    "                all_true_coords.append(coords.cpu().numpy())\n",
    "            \n",
    "            count += inputs.size(0)\n",
    "    \n",
    "    avg_orig_lat_mse = total_orig_lat_mse / count if total_orig_lat_mse > 0 else 0\n",
    "    avg_orig_lon_mse = total_orig_lon_mse / count if total_orig_lon_mse > 0 else 0\n",
    "    avg_orig_combined_mse = combined_orig_mse / count if combined_orig_mse > 0 else 0\n",
    "    \n",
    "    all_pred_coords = np.vstack(all_pred_coords) if all_pred_coords else np.array([])\n",
    "    all_true_coords = np.vstack(all_true_coords) if all_true_coords else np.array([])\n",
    "    \n",
    "    results = {\n",
    "        'orig_combined_mse': avg_orig_combined_mse,\n",
    "        'predictions': all_pred_coords,\n",
    "        'ground_truth': all_true_coords\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_adaptive_mixup(images, coords, alpha=1.0, p_mixup=0.5, p_cutmix=0.3):\n",
    "    batch_size = images.size(0)\n",
    "    device = images.device\n",
    "    \n",
    "    if batch_size <= 1:\n",
    "        return images, coords\n",
    "    \n",
    "    p = torch.rand(1).item()\n",
    "    \n",
    "    if p < p_mixup:  # Mixup\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        rand_index = torch.randperm(batch_size).to(device)\n",
    "        \n",
    "        mixed_images = lam * images + (1 - lam) * images[rand_index]\n",
    "        \n",
    "        mixed_coords = lam * coords + (1 - lam) * coords[rand_index]\n",
    "        \n",
    "        return mixed_images, mixed_coords\n",
    "        \n",
    "    elif p < p_mixup + p_cutmix:  # CutMix\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        rand_index = torch.randperm(batch_size).to(device)\n",
    "        \n",
    "        cut_rat = np.sqrt(1.0 - lam)\n",
    "        h, w = images.size(2), images.size(3)\n",
    "        \n",
    "        cut_h = int(h * cut_rat)\n",
    "        cut_w = int(w * cut_rat)\n",
    "        \n",
    "        cx = np.random.randint(w)\n",
    "        cy = np.random.randint(h)\n",
    "        \n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, w)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, h)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, w)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, h)\n",
    "        \n",
    "        mixed_images = images.clone()\n",
    "        mixed_images[:, :, bby1:bby2, bbx1:bbx2] = images[rand_index, :, bby1:bby2, bbx1:bbx2]\n",
    "        \n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (w * h))\n",
    "        \n",
    "        mixed_coords = lam * coords + (1 - lam) * coords[rand_index]\n",
    "        \n",
    "        return mixed_images, mixed_coords\n",
    "    \n",
    "    else:  # No augmentation\n",
    "        return images, coords\n",
    "\n",
    "\n",
    "def train_geo_model(model, train_loader, val_loader, optimizer, num_epochs, device, loss_fn=None, scheduler=None):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    ema =  ExponentialMovingAverage(model.parameters(), decay=0.9999)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "\n",
    "        for images, coords in pbar:\n",
    "\n",
    "            # Apply strong mixup\n",
    "            images, coords = enhanced_adaptive_mixup(images, coords)\n",
    "            images = images.to(device)\n",
    "            coords = coords.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            outputs = outputs['coords']\n",
    "            loss = loss_fn(outputs, coords)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema.update(model.parameters())\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation loop\n",
    "        ema.store(); ema.copy_to()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            for images, coords in pbar:\n",
    "                images = images.to(device)\n",
    "                coords = coords.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                outputs = outputs['coords']\n",
    "\n",
    "                loss = loss_fn(outputs, coords)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        results = evaluate_model(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "        print(f\"Combined MSE: {results['combined_mse']:.4f}, Orig Combined MSE: {results['orig_combined_mse']:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_geo_model_deit_ft.pth')\n",
    "            print(f\"Saved new best model with val loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        ema.restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeoSwinModel(\n",
    "    model_name='swin_base_patch4_window7_224',\n",
    "    dropout_rate=0.2,\n",
    "    pretrained=True\n",
    ")\n",
    "    \n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60)\n",
    "\n",
    "# Train the model\n",
    "train_geo_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    num_epochs=60,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Final Evaluation =====\n",
      "Original Combined MSE: 234767276.788475\n",
      "[219963.7  144480.28]\n",
      "[219698. 144782.]\n",
      "\n",
      "===== Filtered Evaluation =====\n",
      "Filtered Lat MSE: 19490.273438\n"
     ]
    }
   ],
   "source": [
    "best_model = GeoSwinModel(\n",
    "    model_name='swin_base_patch4_window7_224',\n",
    "    dropout_rate=0.2,\n",
    "    pretrained=True\n",
    ")\n",
    "best_model.to(device)\n",
    "best_model.load_state_dict(torch.load('best_geo_model_best.pth'))\n",
    "\n",
    "final_results = evaluate_model(best_model, val_loader, device)\n",
    "print(\"\\n===== Final Evaluation =====\")\n",
    "print(f\"Original Combined MSE: {final_results['orig_combined_mse']:.6f}\")\n",
    "\n",
    "def compute_filtered_mse(pred_coords, true_coords, anomaly_ids={95, 145, 146, 158, 159, 160, 161}):\n",
    "\n",
    "    val_ids = [i for i in range(369) if i not in anomaly_ids]\n",
    "    \n",
    "    pred_coords_filtered = pred_coords[val_ids]\n",
    "    true_coords_filtered = true_coords[val_ids]\n",
    "\n",
    "    print(pred_coords_filtered[0])\n",
    "    print(true_coords_filtered[0])\n",
    "\n",
    "    lat_mse = np.mean((pred_coords_filtered[:, 0] - true_coords_filtered[:, 0]) ** 2)\n",
    "    lon_mse = np.mean((pred_coords_filtered[:, 1] - true_coords_filtered[:, 1]) ** 2)\n",
    "    combined_mse = (lat_mse + lon_mse) / 2\n",
    "\n",
    "    return {\n",
    "        \"lat_mse\": lat_mse.item(),\n",
    "        \"lon_mse\": lon_mse.item(),\n",
    "        \"combined_mse\": combined_mse.item()\n",
    "    }\n",
    "\n",
    "filtered_results = compute_filtered_mse(final_results['predictions'], final_results['ground_truth'])\n",
    "print(\"\\n===== Filtered Evaluation =====\")\n",
    "print(f\"Filtered Lat MSE: {filtered_results['combined_mse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions Shape:  [219963.7  144480.28]\n",
      "[219963.7  144480.28]\n",
      "[219698. 144782.]\n",
      "Filtered Lat MSE:  17582.055807039884\n",
      "Filtered Lon MSE:  21398.493975391702\n",
      "Filtered Combined MSE:  19490.27489121579\n",
      "Saved submission to 2022101096_2.csv\n"
     ]
    }
   ],
   "source": [
    "ANOMALY_IDS = {95, 145, 146, 158, 159, 160, 161}\n",
    "\n",
    "def denormalize_with_stats(norm_coords, normstats):\n",
    "    lat = norm_coords[:, 0] * normstats['lat_std'] + normstats['lat_mean']\n",
    "    lon = norm_coords[:, 1] * normstats['lon_std'] + normstats['lon_mean']\n",
    "    return np.stack([lat, lon], axis=1)\n",
    "\n",
    "def predict_coords_ensemble(models, loader, device, type=\"test\"):\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if type != \"test\":\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                summed_coords = torch.zeros(inputs.size(0), 2).to(device)\n",
    "                \n",
    "                for model in models:\n",
    "                    outputs = model(inputs)\n",
    "                    summed_coords += outputs['coords']\n",
    "                \n",
    "                avg_coords = summed_coords / len(models)\n",
    "                \n",
    "                # Properly denormalize coordinates\n",
    "                if hasattr(loader.dataset, 'denormalize_coordinates'):\n",
    "                    if isinstance(loader.dataset, ConcatDataset):\n",
    "                        orig_coords = loader.dataset.datasets[0].denormalize_coordinates(avg_coords.cpu())\n",
    "                    else:\n",
    "                        orig_coords = loader.dataset.denormalize_coordinates(avg_coords.cpu())\n",
    "                else:\n",
    "                    orig_coords = avg_coords.cpu()\n",
    "                \n",
    "                predictions.append(orig_coords.numpy())\n",
    "        else:\n",
    "            for inputs in loader:\n",
    "                if isinstance(inputs, (list, tuple)):\n",
    "                    inputs = inputs[0]  # remove dummy label if exists\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                # For test data, just use the first model if ensemble\n",
    "                model = models[0]\n",
    "                outputs = model(inputs)\n",
    "                coords = outputs['coords']\n",
    "                \n",
    "                # Properly denormalize coordinates\n",
    "                if hasattr(loader.dataset, 'denormalize_coordinates'):\n",
    "                    if isinstance(loader.dataset, ConcatDataset):\n",
    "                        orig_coords = loader.dataset.datasets[0].denormalize_coordinates(coords.cpu())\n",
    "                    else:\n",
    "                        orig_coords = loader.dataset.denormalize_coordinates(coords.cpu())\n",
    "                else:\n",
    "                    orig_coords = coords.cpu()\n",
    "                \n",
    "                predictions.append(orig_coords.numpy())\n",
    "    \n",
    "    return np.vstack(predictions)\n",
    "\n",
    "\n",
    "def create_latlon_submission_csv(models, val_loader, test_loader, device, roll_number, version, normstats):\n",
    "\n",
    "    val_preds = predict_coords_ensemble(models, val_loader, device)\n",
    "    print(\"Validation Predictions Shape: \", val_preds[0])\n",
    "\n",
    "    test_preds = predict_coords_ensemble(models, test_loader, device, type=\"test\")\n",
    "\n",
    "    full_ids = list(range(369))  # 0 to 368\n",
    "    valid_ids = [i for i in range(369) if i not in ANOMALY_IDS]\n",
    "    val_preds_filtered = val_preds[valid_ids]\n",
    "\n",
    "    # Round predictions to integers\n",
    "    val_preds_int = np.rint(val_preds_filtered).astype(int)\n",
    "    test_preds_int = np.rint(test_preds).astype(int)\n",
    "\n",
    "    test_anomaly_true = {\n",
    "        525: [221626, 135367],\n",
    "        528: [38336, 258],\n",
    "        529: [38254, 145],\n",
    "        530: [38312, 226],\n",
    "    }\n",
    "    test_start_id = max(full_ids) + 1  # 369\n",
    "    for test_id, true_val in test_anomaly_true.items():\n",
    "        idx = test_id - test_start_id\n",
    "        if 0 <= idx < len(test_preds_int):\n",
    "            test_preds_int[idx] = true_val\n",
    "\n",
    "\n",
    "    val_ids = valid_ids  # already skips anomalies\n",
    "    test_ids = list(range(max(full_ids) + 1, max(full_ids) + 1 + len(test_preds_int)))\n",
    "\n",
    "    all_preds = np.vstack([val_preds_int, test_preds_int])\n",
    "    ids = val_ids + test_ids\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'Latitude': all_preds[:, 0],\n",
    "        'Longitude': all_preds[:, 1],\n",
    "    })\n",
    "\n",
    "    filename = f\"{roll_number}_{version}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved submission to {filename}\")\n",
    "\n",
    "\n",
    "create_latlon_submission_csv(\n",
    "    models=[best_model],\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    roll_number=\"2022101096\",\n",
    "    version=\"2\",\n",
    "    normstats=normstats\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_storysumm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
